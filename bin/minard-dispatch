#!/usr/bin/env python
from __future__ import print_function, division
import time
from redis import Redis
from dispatch import Dispatch, unpack_trigger_type, unpack_header, unpack_pmt_record, RECORD_IDS
from collections import defaultdict
import threading
from minard.timeseries import INTERVALS, EXPIRE, HASH_INTERVALS, HASH_EXPIRE
from minard.redistools import hmincrby, hdivk, setavgmax
import random
from snotdaq import Logger

# Set up the logging
log = Logger()

redis = Redis()

# triggers, note: the order here is important!
# the position of the trigger in the list corresponds to the bit in the
# trigger word.
# http://snopl.us/docs/rat/user_manual/html/node43.html
TRIGGER_NAMES = \
['100L',
 '100M',
 '100H',
 '20',
 '20LB',
 'ESUML',
 'ESUMH',
 'OWLN',
 'OWLEL',
 'OWLEH',
 'PULGT',
 'PRESCL',
 'PED',
 'PONG',
 'SYNC',
 'EXTA',
 'EXT2',
 'EXT3',
 'EXT4',
 'EXT5',
 'EXT6',
 'EXT7',
 'EXT8',
 'SRAW',
 'NCD',
 'SOFGT',
 'MISS']

def post(host):
    """Posts the dispatcher name to redis every 20 seconds."""
    redis.setex('dispatcher',host,60)
    timer = threading.Timer(20.0, post, args=[host])
    timer.daemon = True
    timer.start()

def flush_cache(cache, cache_set, cache_nhit, cache_pmt, time):
    # for docs on redis pipeline see http://redis.io/topics/pipelining
    p = redis.pipeline()

    for name, hash in cache.items():
        if isinstance(hash, dict):
            keys = ['ts:%i:%i:%s' % (interval, time//interval, name)
                    for interval in INTERVALS]

            if len(hash) > 0:
                hmincrby(keys, hash, client=p)

                for key, interval in zip(keys,INTERVALS):
                    p.expire(key,interval*EXPIRE)
        else:
            for interval in INTERVALS:
                key = 'ts:%i:%i:%s' % (interval, time//interval, name)
                p.incrby(key, hash)
                p.expire(key,interval*EXPIRE)

    for interval in INTERVALS:
        for name, hash in cache_set.items():
            key = 'ts:%i:%i:%s' % (interval, time//interval, name)
            if len(hash) > 0:
                p.hmset(key, hash)
                p.expire(key, interval*EXPIRE)

    keys = ['ts:%i:%i:occupancy:hits' % (interval, time//interval)
            for interval in HASH_INTERVALS]

    if len(cache_pmt) > 0:
        hmincrby(keys, cache_pmt, client=p)

    for interval in HASH_INTERVALS:
        key = 'ts:%i:%i:occupancy' % (interval, time//interval)
        p.incrby(key + ':count', cache['trig']['TOTAL'])
        # expire after just interval, because these will
        # occupancy will be set as hits/count
        p.expire(key + ':hits', interval)
        p.expire(key + ':count', interval)

        prev_key = 'ts:%i:%i:occupancy' % (interval,time//interval-1)
        if redis.incr(prev_key + ':lock') == 1:
            hdivk(prev_key, prev_key + ':hits', prev_key + ':count',
                  range(10240), format='%.2g', client=p)
            keys = setavgmax(prev_key, client=p)
            for k in keys:
                p.expire(k, HASH_EXPIRE*interval)
            p.expire(prev_key, HASH_EXPIRE*interval)
            p.expire(prev_key + ':lock', interval)

    for trigger, cache in cache_nhit.iteritems():
        if len(cache) > 0:
            # nhit distribution
            if len(cache) > 100:
                # if there are more than 100 events this second
                # randomly sample the nhit from 100 events
                cache = random.sample(cache,100)
            # see http://flask.pocoo.org/snippets/71/ for this design pattern
            p.lpush('ts:1:%i:nhit:%s' % (time, trigger), *cache)
            p.expire('ts:1:%i:nhit:%s' % (time, trigger), 3600)

    p.execute()

def pull(host, num_workers, worker):
    """Connects to the dispatcher and processes the dispatch stream."""
    dispatcher = Dispatch(host)

    log.notice("connected to %s" % host)

    post(host)

    cache = {}
    cache['trig'] = defaultdict(int)
    cache['trig:nhit'] = defaultdict(int)
    cache['trig:charge'] = defaultdict(int)
    cache['trig:fecd'] = defaultdict(int)
    cache['DISPATCH_ORPHANS'] = 0
    cache_set = {}
    cache_set['trig'] = {}
    cache_nhit = defaultdict(list)
    cache_pmt = defaultdict(int)

    then = None

    while True:
        record = dispatcher.recv(block=False)
        now = int(time.time())

        if then is None:
            then = now

        if now > then:
            # flush data to redis every second
            flush_cache(cache, cache_set, cache_nhit, cache_pmt, then)

            p = redis.pipeline()
            for interval in INTERVALS:
                key = 'ts:%i:%i:heartbeat' % (interval, then//interval)
                p.setex(key,1,interval*EXPIRE)
            p.execute()

            cache['trig'].clear()
            cache['trig:nhit'].clear()
            cache['trig:charge'].clear()
            cache['trig:fecd'].clear()
            cache['DISPATCH_ORPHANS'] = 0
            cache_set['trig'].clear()
            cache_nhit.clear()
            cache_pmt.clear()
            then = now

        if record is None:
            # nothing to process, take a break
            log.debug("timeout")
            time.sleep(0.01)
            continue

        record_id, data = unpack_header(record.raw)

        if record_id != RECORD_IDS['PMT_RECORD']:
            continue

        pmt_gen = unpack_pmt_record(data)

        pev = next(pmt_gen)

        run = pev.RunNumber
        gtid = pev.TriggerCardData.BcGT

        if gtid % num_workers != worker:
            continue

        nhit = pev.NPmtHit
        subrun = pev.DaqStatus # seriously :)
        trig = unpack_trigger_type(pev)

        nhit = 0

        qhs_sum = 0
        for pmt in pmt_gen:
            id = 16*32*pmt.CrateID + 32*pmt.BoardID + pmt.ChannelID
            cache_pmt[id] += 1

            if pmt.CrateID == 17 and pmt.BoardID == 15:
                if pmt.ChannelID == 4:
                    cache['trig:fecd']['N16'] += 1
                if pmt.ChannelID == 17:
                    cache['trig:fecd']['20LB'] += 1
                elif pmt.ChannelID == 19:
                    cache['trig:fecd']['20'] += 1
                elif pmt.ChannelID == 28:
                    cache['trig:fecd']['100L'] += 1
                elif pmt.ChannelID == 29:
                    cache['trig:fecd']['100M'] += 1
                elif pmt.ChannelID == 31:
                    cache['trig:fecd']['100H'] += 1

                # don't include FEC/D in qhs sum and nhit
                continue

            nhit += 1

            qhs_sum += pmt.Qhs

        if trig == 0:
            # orphan
            cache['DISPATCH_ORPHANS'] += nhit
            continue

        cache_nhit['all'].append(nhit)

        cache['trig']['TOTAL'] += 1
        cache['trig:nhit']['TOTAL'] += nhit
        cache['trig:charge']['TOTAL'] += qhs_sum
        cache_set['trig']['run'] = run
        cache_set['trig']['subrun'] = subrun
        cache_set['trig']['gtid'] = gtid

        for i, name in enumerate(TRIGGER_NAMES):
            if trig & (1 << i):
                cache['trig'][i] += 1
                cache['trig:nhit'][i] += nhit
                cache['trig:charge'][i] += qhs_sum
                cache_nhit[name].append(nhit)

def daemonize():
    if os.fork() != 0:
        os._exit(0)

    os.setsid()

    f = open('/dev/null', 'w')

    fd = f.fileno()

    os.dup2(fd, 0)
    os.dup2(fd, 1)
    os.dup2(fd, 2)

if __name__ == '__main__':
    import sys
    import argparse

    parser = argparse.ArgumentParser(description='Process SNO+ events from a dispatch stream and update redis')
    parser.add_argument('--host', default='localhost',
                        help='hostname of the dispatcher')
    parser.add_argument('-n', '--num-workers', type=int, default=1,
                        help='total number of workers running')
    parser.add_argument('-w', '--worker', type=int, default=0,
                        help='the worker number. This means the script will only process GTIDs which are equal to the worker number mod the total number of workers.')
    parser.add_argument("--logfile", help="filename for log file",
                        default=None)
    parser.add_argument("-d", "--daemonize", action="store_true",
			default=False)
    parser.add_argument('--loglevel',
                        help="logging level (debug, verbose, notice, warning)",
                        default='notice')
    args = parser.parse_args()

    log.set_verbosity(args.loglevel)

    if args.logfile:
        log.set_logfile(args.logfile)

    if args.daemonize:
	daemonize()

    if args.worker >= args.num_workers:
        log.warn("worker number must be less than the number of workers")
        sys.exit(1)

    while True:
	try:
	    pull(args.host, args.num_workers, args.worker)
	except Exception as e:
	    log.warn("unhandled exception: %s" % str(e))
            time.sleep(60)
	    continue
